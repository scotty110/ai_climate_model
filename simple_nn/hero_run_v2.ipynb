{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hero Run for Training Simple Feed Forward Ensamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not sure how mpi will be calling the models, but potentially x72 models. I wanted to have an ensamble of 3 models to calc the average output. So need x216 models, at 200 MB is 43.2 GB. Further investigation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.amp import autocast\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "import h5py as h5\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do hybrid heterogenious computing, so assert GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available(), 'CUDA is not available.'\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple FF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple FF net. Flatten input so $(3*2*2 + 70*2*2*2)=852$. And flatten output $(70*2*2*2)=560$.\n",
    "Also use float32 to minimize size (can move to float64 later), CESM will output float64 so will need to handle inside the ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(852,4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 560)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was pre partitioned into training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name = join('/home/squirt/Documents/data/weather_data/', 'train_data.h5')\n",
    "test_name = join('/home/squirt/Documents/data/weather_data/', 'test_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to load hdf5 file of the processed weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5(filename:str):\n",
    "    '''\n",
    "    Load data from an HDF5 file and return a list of dictionaries.\n",
    "    Inputs:\n",
    "        - filename (str): Path to the HDF5 file.\n",
    "    Outputs:\n",
    "        - data (list): List of dictionaries, where each dictionary represents an entry in the original list.\n",
    "    '''\n",
    "    data = []  # List to hold dictionaries\n",
    "    with h5.File(filename, 'r') as f:\n",
    "        # Iterate through groups (each representing an entry in the original list)\n",
    "        for group_name in f:\n",
    "            group = f[group_name]\n",
    "            # Reconstruct dictionary from datasets and attributes\n",
    "            entry = {\n",
    "                # Attributes (metadata)\n",
    "                'day': group.attrs['day'],\n",
    "                'region': group.attrs['region'],\n",
    "                'time': group.attrs['time'],\n",
    "\n",
    "                # groups (numpy arrays)\n",
    "                'landmass': group['landmass'][...],  # Use [...] to read the full dataset\n",
    "                'x': group['x'][...],\n",
    "                'y': group['y'][...],\n",
    "            }\n",
    "            data.append(entry)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate stacks to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_data(data:list[dict], key:str) -> torch.Tensor:\n",
    "    return torch.stack([torch.tensor(entry[key]) for entry in data])\n",
    "\n",
    "\n",
    "def generate_stacks(data:list[dict]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Create a PyTorch DataLoader from the data.\n",
    "    Inputs:\n",
    "        - data (list): List of dictionaries, where each dictionary represents an entry in the original list.\n",
    "    Outputs:\n",
    "        - landmass (torch.Tensor): Tensor of landmass data.\n",
    "        - x (torch.Tensor): Tensor of x-coordinate data.\n",
    "        - y (torch.Tensor): Tensor of y-coordinate data.\n",
    "    '''\n",
    "    landmass = stack_data(data, 'landmass')\n",
    "\n",
    "    x = stack_data(data, 'x')\n",
    "    x = x.transpose(2, 1)\n",
    "\n",
    "    y = stack_data(data, 'y')\n",
    "    y = y.transpose(2, 1)\n",
    "    \n",
    "    return (landmass, x, y)\n",
    "\n",
    "\n",
    "class weather_dataset(Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset class for weather data.\n",
    "    '''\n",
    "    def __init__(self, data:list[dict]):\n",
    "        self.landmass, self.x, self.y = generate_stacks(data)\n",
    "        self.length = len(self.landmass)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.landmass[idx], self.x[idx], self.y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition training data into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(fname:str, batch_size:int, split:int) -> tuple[DataLoader, DataLoader]:\n",
    "    '''\n",
    "    Create PyTorch DataLoader objects for training and validation data.\n",
    "    Inputs:\n",
    "        - fname (str): Path to the HDF5 file.\n",
    "        - batch_size (int): Batch size for the DataLoader objects.\n",
    "        - split (float): Fraction of the data to use for training.     \n",
    "    Outputs:\n",
    "        - train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "        - test_loader (torch.utils.data.DataLoader): DataLoader for test data.\n",
    "    '''\n",
    "    # Load data and create tensor \n",
    "    data = load_hdf5(fname)\n",
    "    dataset = weather_dataset(data)\n",
    "    \n",
    "    train_size = int(split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Normalize the x/y data, not the landmass, on train_loader.dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to have normalization layer (x) -> reshape -> NN -> reshape - > denorm layer (y). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizationLayer(nn.Module):\n",
    "    def __init__(self, data: torch.Tensor):\n",
    "        super(NormalizationLayer, self).__init__()\n",
    "\n",
    "        # Compute mean and std along the batch_size, x1, and x2 dimensions\n",
    "        self.mean = data.mean(dim=(0, 3, 4), keepdim=True).to(device, dtype=dtype) \n",
    "        self.std = data.std(dim=(0, 3, 4), keepdim=True).to(device, dtype=dtype) \n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        # Normalize the data\n",
    "        normalized_data = (x - self.mean) / (self.std + 1e-9)  # Add a small constant to avoid division by zero\n",
    "        return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenormalizationLayer(nn.Module):\n",
    "    def __init__(self, data: torch.Tensor):\n",
    "        super(DenormalizationLayer, self).__init__()\n",
    "\n",
    "        # Compute mean and std along the batch_size, x1, and x2 dimensions\n",
    "        self.mean = data.mean(dim=(0, 3, 4), keepdim=True).to(device, dtype=dtype)  \n",
    "        self.std = data.std(dim=(0, 3, 4), keepdim=True).to(device, dtype=dtype) \n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        # Denormalize the data\n",
    "        denormalized_data = x * (self.std + 1e-9) + self.mean\n",
    "        return denormalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(nn.Module):\n",
    "    def __init__(self, l_shape: tuple = (-1, 3*2*2), x_shape: tuple = (-1, 70*3*2*2), y_shape: tuple = (-1, 70,2,2,2)):\n",
    "        super(ReshapeLayer, self).__init__()\n",
    "        self.l_shape = l_shape\n",
    "        self.x_shape = x_shape\n",
    "        self.y_shape = y_shape\n",
    "\n",
    "    def reshape_x(self, l: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        l = l.view(self.l_shape)\n",
    "        x = x.view(self.x_shape)\n",
    "        x = torch.cat((l, x), 1)\n",
    "        return x\n",
    "\n",
    "    def reshape_y(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        y = y.view(self.y_shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteModel(nn.Module):\n",
    "    def __init__(self, norm_layer:NormalizationLayer, reshape_layer:ReshapeLayer, neural_network:simpleNN, denorm_layer:DenormalizationLayer):\n",
    "        super(CompleteModel, self).__init__()\n",
    "        self.norm_layer = norm_layer\n",
    "        self.reshape_layer = reshape_layer \n",
    "        self.neural_network = neural_network\n",
    "        self.denorm_layer = denorm_layer\n",
    "\n",
    "    def forward(self, x:torch.Tensor, l:torch.tensor) -> torch.Tensor:\n",
    "        x = self.norm_layer(x)\n",
    "        x = self.reshape_layer.reshape_x(l, x)\n",
    "\n",
    "        x = self.neural_network(x)\n",
    "        x = self.reshape_layer.reshape_y(x)  # reshape back to original shape\n",
    "        x = self.denorm_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Norm/Denorm x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(loader) -> CompleteModel:\n",
    "    norm_layer = NormalizationLayer(loader.dataset.dataset.x)\n",
    "    denorm_layer = DenormalizationLayer(loader.dataset.dataset.y)\n",
    "    reshape_layer = ReshapeLayer().to(device, dtype=dtype) \n",
    "\n",
    "    model = simpleNN().to(device, dtype=dtype) \n",
    "\n",
    "    network = CompleteModel(norm_layer, reshape_layer, model, denorm_layer).to(device, dtype=dtype) \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training loop, use mixed percision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module, dl:torch.utils.data.DataLoader, optim:torch.optim, loss:nn.Module) -> float:\n",
    "    model.train()\n",
    "    total_loss = .0\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for _, (l, x, y) in enumerate(dl):\n",
    "        l = l.to(device, dtype=dtype) \n",
    "        x = x.to(device, dtype=dtype) \n",
    "        y = y.to(device, dtype=dtype) \n",
    "\n",
    "        optim.zero_grad()\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            y_pred = model(x, l)\n",
    "            l = loss(y_pred, y)\n",
    "            total_loss += l.item()\n",
    "\n",
    "        # Preform backpass\n",
    "        scaler.scale(l).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "    \n",
    "    return total_loss / len(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model:nn.Module, dl:torch.utils.data.DataLoader, loss:nn.Module) -> float:\n",
    "    model.eval()\n",
    "    total_loss = .0\n",
    "\n",
    "    for _, (l, x, y) in enumerate(dl):\n",
    "        l = l.to(device, dtype=dtype) \n",
    "        x = x.to(device, dtype=dtype) \n",
    "        y = y.to(device, dtype=dtype)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            y_pred = model(x, l)\n",
    "            l = loss(y_pred, y)\n",
    "            total_loss += l.item()\n",
    "\n",
    "    return total_loss / len(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3 \n",
    "models = {}\n",
    "\n",
    "test_ds = weather_dataset(load_hdf5(test_name)) \n",
    "test_loader = DataLoader(test_ds, batch_size=4096, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "for i in range(folds):\n",
    "    train_loader, val_loader = get_dataloaders(train_name, 4096, 0.6)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = make_model(train_loader) \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Early Stopping \n",
    "    eval_loss = -1*float('inf') \n",
    "    train_loss  = float('inf') \n",
    "    j = 0\n",
    "    print(f'Fold {i+1}')\n",
    "    while train_loss > eval_loss:\n",
    "        prev_eval_loss = eval_loss\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn)\n",
    "        eval_loss = eval(model, val_loader, loss_fn)\n",
    "        print(f'\\tEpoch {j} - Eval Loss: {eval_loss}')\n",
    "        \n",
    "        if abs(eval_loss - prev_eval_loss) < 0.01:\n",
    "            print(f'\\tEarly stopping at epoch {j} due to minimal change in eval loss')\n",
    "            break\n",
    "        j += 1\n",
    "\n",
    "    # Test Loss\n",
    "    test_loss = eval(model, test_loader, loss_fn)\n",
    "    print(f'Fold {i+1} - Test Loss: {test_loss}')\n",
    "\n",
    "    # Save Model\n",
    "    model.eval()\n",
    "    models[i] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_data(t:torch.tensor) -> torch.tensor:\n",
    "    t = t.view(-1, 70, 2, 2, 2)\n",
    "    # T and QV are the 2 STD channels with a 2x2 grid\n",
    "    # We want to get the top left cell for both channels\n",
    "    p = t[:,:,:,0,0]\n",
    "    p = p.view(-1, 70, 2)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_column(data_dict:dict, idx:int):\n",
    "    if idx not in {0,1}:\n",
    "        raise ValueError('idx must be 0 or 1')\n",
    "    title = 'Temp' if idx == 0 else 'QV'\n",
    "\n",
    "    # Get the data for the 1st column (were just trying to determin if we want to park model for now)\n",
    "    data = []\n",
    "    for _,v in data_dict.items():\n",
    "        data.append(v[0][0,:,idx])\n",
    "\n",
    "    #print(data[0][:,0])\n",
    "    data = torch.stack(data)\n",
    "    #print(data[0,:,0])\n",
    "\n",
    "    # Calculate Mean and STD\n",
    "    mean = (data.mean(dim=0)).cpu().numpy()\n",
    "    std = (data.std(dim=0)).cpu().numpy()\n",
    "\n",
    "    # Get Real Values\n",
    "    real = (data_dict[0][1][0,:,idx]).cpu().numpy()\n",
    "\n",
    "    # Plot\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = range(len(mean))\n",
    "            \n",
    "    plt.plot(x, real, label='True Value', color='blue', linestyle='None', marker='o')\n",
    "    plt.plot(x, mean, label='Mean Prediction', color='red', linestyle='--')\n",
    "    plt.fill_between(x, mean - std, mean + std, color='gray', alpha=0.2, label='Prediction Std Dev')\n",
    "\n",
    "    plt.title(f'{title} Predictions vs True Values')\n",
    "    plt.xlabel('Cell')\n",
    "    plt.ylabel(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(test_loader))\n",
    "l, x, y = batch_data\n",
    "l = l.cuda()\n",
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "\n",
    "# Flatten and combine\n",
    "l = l.view(-1, 3*2*2)\n",
    "x = x.view(-1, 70*3*2*2)\n",
    "x = torch.cat((l, x), 1)\n",
    "\n",
    "y = y.view(-1, 70*2*2*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = {}\n",
    "for i, model in models.items():\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(x)\n",
    "        model_preds[i] = (y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values = {}\n",
    "for k,v in model_preds.items():\n",
    "    plot_values[k] = (get_cell_data(v[0]), get_cell_data(v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(plot_values, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(plot_values, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in models.items():\n",
    "    try:\n",
    "        script_model = torch.jit.script(m)\n",
    "        script_model.save(f\"./weights/simple_net_{i}.pt\")\n",
    "        print(f\"Model {i} saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model {i}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
