{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hero Run for Training Simple Feed Forward Ensamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not sure how mpi will be calling the models, but potentially x72 models. I wanted to have an ensamble of 3 models to calc the average output. So need x216 models, at 200 MB is 43.2 GB. Further investigation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.amp import autocast\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "import h5py as h5\n",
    "\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do hybrid heterogenious computing, so assert GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available(), 'CUDA is not available.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple FF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple FF net. Flatten input so $(3*2*2 + 70*2*2*2)=852$. And flatten output $(70*2*2*2)=560$.\n",
    "Also use float32 to minimize size (can move to float64 later), CESM will output float64 so will need to handle inside the ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(852,4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 560)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleNN()\n",
    "model = model.double().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was pre partitioned into training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name = join('/home/squirt/Documents/data/weather_data/', 'train_data.h5')\n",
    "test_name = join('/home/squirt/Documents/data/weather_data/', 'test_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to load hdf5 file of the processed weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5(filename:str):\n",
    "    '''\n",
    "    Load data from an HDF5 file and return a list of dictionaries.\n",
    "    Inputs:\n",
    "        - filename (str): Path to the HDF5 file.\n",
    "    Outputs:\n",
    "        - data (list): List of dictionaries, where each dictionary represents an entry in the original list.\n",
    "    '''\n",
    "    data = []  # List to hold dictionaries\n",
    "    with h5.File(filename, 'r') as f:\n",
    "        # Iterate through groups (each representing an entry in the original list)\n",
    "        for group_name in f:\n",
    "            group = f[group_name]\n",
    "            # Reconstruct dictionary from datasets and attributes\n",
    "            entry = {\n",
    "                # Attributes (metadata)\n",
    "                'day': group.attrs['day'],\n",
    "                'region': group.attrs['region'],\n",
    "                'time': group.attrs['time'],\n",
    "\n",
    "                # groups (numpy arrays)\n",
    "                'landmass': group['landmass'][...],  # Use [...] to read the full dataset\n",
    "                'x': group['x'][...],\n",
    "                'y': group['y'][...],\n",
    "            }\n",
    "            data.append(entry)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate stacks to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_data(data:list[dict], key:str) -> torch.Tensor:\n",
    "    return torch.stack([torch.tensor(entry[key]) for entry in data])\n",
    "\n",
    "\n",
    "def generate_stacks(data:list[dict]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Create a PyTorch DataLoader from the data.\n",
    "    Inputs:\n",
    "        - data (list): List of dictionaries, where each dictionary represents an entry in the original list.\n",
    "    Outputs:\n",
    "        - landmass (torch.Tensor): Tensor of landmass data.\n",
    "        - x (torch.Tensor): Tensor of x-coordinate data.\n",
    "        - y (torch.Tensor): Tensor of y-coordinate data.\n",
    "    '''\n",
    "    landmass = stack_data(data, 'landmass')\n",
    "\n",
    "    x = stack_data(data, 'x')\n",
    "    x = x.transpose(2, 1)\n",
    "\n",
    "    y = stack_data(data, 'y')\n",
    "    y = y.transpose(2, 1)\n",
    "    \n",
    "    return (landmass, x, y)\n",
    "\n",
    "\n",
    "class weather_dataset(Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset class for weather data.\n",
    "    '''\n",
    "    def __init__(self, data:list[dict]):\n",
    "        self.landmass, self.x, self.y = generate_stacks(data)\n",
    "        self.length = len(self.landmass)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.landmass[idx], self.x[idx], self.y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition training data into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(fname:str, batch_size:int, split:int) -> tuple[DataLoader, DataLoader]:\n",
    "    '''\n",
    "    Create PyTorch DataLoader objects for training and validation data.\n",
    "    Inputs:\n",
    "        - fname (str): Path to the HDF5 file.\n",
    "        - batch_size (int): Batch size for the DataLoader objects.\n",
    "        - split (float): Fraction of the data to use for training.     \n",
    "    Outputs:\n",
    "        - train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "        - test_loader (torch.utils.data.DataLoader): DataLoader for test data.\n",
    "    '''\n",
    "    # Load data and create tensor \n",
    "    data = load_hdf5(fname)\n",
    "    dataset = weather_dataset(data)\n",
    "    \n",
    "    train_size = int(split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training loop, use mixed percision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module, dl:torch.utils.data.DataLoader, optim:torch.optim, loss:nn.Module) -> float:\n",
    "    model.train()\n",
    "    total_loss = .0\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for _, (l, x, y) in enumerate(dl):\n",
    "        l = l.cuda()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        # Flatten and combine\n",
    "        l = l.view(-1, 3*2*2)\n",
    "        x = x.view(-1, 70*3*2*2)\n",
    "        x = torch.cat((l, x), 1)\n",
    "\n",
    "        y = y.view(-1, 70*2*2*2)\n",
    "\n",
    "        # Forward pass\n",
    "        optim.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            y_pred = model(x)\n",
    "            l = loss(y_pred, y)\n",
    "            total_loss += l.item()\n",
    "\n",
    "        # Preform backpass\n",
    "        scaler.scale(l).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "    \n",
    "    return total_loss / len(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model:nn.Module, dl:torch.utils.data.DataLoader, loss:nn.Module) -> float:\n",
    "    model.eval()\n",
    "    total_loss = .0\n",
    "\n",
    "    for _, (l, x, y) in enumerate(dl):\n",
    "        l = l.cuda()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        # Flatten and combine\n",
    "        l = l.view(-1, 3*2*2)\n",
    "        x = x.view(-1, 70*3*2*2)\n",
    "        x = torch.cat((l, x), 1)\n",
    "\n",
    "        y = y.view(-1, 70*2*2*2)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            y_pred = model(x)\n",
    "            l = loss(y_pred, y)\n",
    "            total_loss += l.item()\n",
    "\n",
    "    return total_loss / len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\tEpoch 0 - Eval Loss: 6115844.662589727\n",
      "\tEpoch 1 - Eval Loss: 2723509.928000697\n",
      "\tEpoch 2 - Eval Loss: 98147.90964271483\n",
      "\tEpoch 3 - Eval Loss: 20941.188989376627\n",
      "\tEpoch 4 - Eval Loss: 3668.0223015239662\n",
      "\tEpoch 5 - Eval Loss: 1092.7534572182558\n",
      "\tEpoch 6 - Eval Loss: 1305.1324568142782\n",
      "\tEpoch 7 - Eval Loss: 259.41750121681406\n",
      "\tEpoch 8 - Eval Loss: 264.82237080123025\n",
      "\tEpoch 9 - Eval Loss: 124.40882908244859\n",
      "\tEpoch 10 - Eval Loss: 46.73697941471736\n",
      "\tEpoch 11 - Eval Loss: 92.33288484527591\n",
      "Fold 1 - Test Loss: 92.26096223416238\n",
      "Fold 2\n",
      "\tEpoch 0 - Eval Loss: 8062120.4634836605\n",
      "\tEpoch 1 - Eval Loss: 3502085.809838276\n",
      "\tEpoch 2 - Eval Loss: 132901.56551182678\n",
      "\tEpoch 3 - Eval Loss: 51459.149403144875\n",
      "\tEpoch 4 - Eval Loss: 6949.590930923096\n",
      "\tEpoch 5 - Eval Loss: 2199.3836329882242\n",
      "\tEpoch 6 - Eval Loss: 958.4522179292824\n",
      "\tEpoch 7 - Eval Loss: 715.9168467586009\n",
      "\tEpoch 8 - Eval Loss: 280.82644643218964\n",
      "\tEpoch 9 - Eval Loss: 108.75267219971597\n",
      "\tEpoch 10 - Eval Loss: 86.86728625704839\n",
      "\tEpoch 11 - Eval Loss: 52.668161283119076\n",
      "\tEpoch 12 - Eval Loss: 19.916742842212695\n",
      "\tEpoch 13 - Eval Loss: 5.986527847454377\n",
      "\tEpoch 14 - Eval Loss: 1.8465751894107647\n",
      "\tEpoch 15 - Eval Loss: 0.9025260357139575\n",
      "\tEpoch 16 - Eval Loss: 0.6816299913738544\n",
      "\tEpoch 17 - Eval Loss: 0.6489493679164419\n",
      "\tEpoch 18 - Eval Loss: 0.6314591187195828\n",
      "\tEpoch 19 - Eval Loss: 0.6311111032195922\n",
      "\tEarly stopping at epoch 19 due to minimal change in eval loss\n",
      "Fold 2 - Test Loss: 0.6711856775985172\n",
      "Fold 3\n",
      "\tEpoch 0 - Eval Loss: 6434552.189596845\n",
      "\tEpoch 1 - Eval Loss: 3065729.5843598805\n",
      "\tEpoch 2 - Eval Loss: 104839.55633361905\n",
      "\tEpoch 3 - Eval Loss: 32198.433217435733\n",
      "\tEpoch 4 - Eval Loss: 5453.674430369194\n",
      "\tEpoch 5 - Eval Loss: 1549.868230811087\n",
      "\tEpoch 6 - Eval Loss: 492.0820358985519\n",
      "\tEpoch 7 - Eval Loss: 325.9408595736312\n",
      "\tEpoch 8 - Eval Loss: 118.00756993701629\n",
      "\tEpoch 9 - Eval Loss: 39.511814649661034\n",
      "\tEpoch 10 - Eval Loss: 13.247662116968893\n",
      "\tEpoch 11 - Eval Loss: 3.5628546625360915\n",
      "\tEpoch 12 - Eval Loss: 1.6481305574256622\n",
      "\tEpoch 13 - Eval Loss: 0.7662627648332685\n",
      "\tEpoch 14 - Eval Loss: 0.678549798407776\n",
      "\tEpoch 15 - Eval Loss: 0.6490245813892397\n",
      "\tEpoch 16 - Eval Loss: 0.6572370997785222\n",
      "\tEarly stopping at epoch 16 due to minimal change in eval loss\n",
      "Fold 3 - Test Loss: 0.6806699246264599\n"
     ]
    }
   ],
   "source": [
    "folds = 3 \n",
    "results_dict = {}\n",
    "\n",
    "test_ds = weather_dataset(load_hdf5(test_name)) \n",
    "test_loader = DataLoader(test_ds, batch_size=4096, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "for i in range(folds):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = simpleNN()\n",
    "    model = model.double().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Get Dataloaders\n",
    "    train_loader, val_loader = get_dataloaders(train_name, 4096, 0.6)\n",
    "\n",
    "    # Early Stopping \n",
    "    eval_loss = -1*float('inf') \n",
    "    train_loss  = float('inf') \n",
    "    j = 0\n",
    "    print(f'Fold {i+1}')\n",
    "    while train_loss > eval_loss:\n",
    "        prev_eval_loss = eval_loss\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn)\n",
    "        eval_loss = eval(model, val_loader, loss_fn)\n",
    "        print(f'\\tEpoch {j} - Eval Loss: {eval_loss}')\n",
    "        \n",
    "        if abs(eval_loss - prev_eval_loss) < 0.01:\n",
    "            print(f'\\tEarly stopping at epoch {j} due to minimal change in eval loss')\n",
    "            break\n",
    "        j += 1\n",
    "\n",
    "    \n",
    "    # Test Loss\n",
    "    test_loss = eval(model, test_loader, loss_fn)\n",
    "    print(f'Fold {i+1} - Test Loss: {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
