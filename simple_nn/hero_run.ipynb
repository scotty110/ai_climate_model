{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hero Run for Training Simple Feed Forward Ensamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not sure how mpi will be calling the models, but potentially x72 models. I wanted to have an ensamble of 3 models to calc the average output. So need x216 models, at 200 MB is 43.2 GB. Further investigation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.amp import autocast\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "import h5py as h5\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do hybrid heterogenious computing, so assert GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available(), 'CUDA is not available.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple FF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple FF net. Flatten input so $(3*2*2 + 70*2*2*2)=852$. And flatten output $(70*2*2*2)=560$.\n",
    "Also use float32 to minimize size (can move to float64 later), CESM will output float64 so will need to handle inside the ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(852,4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 560)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleNN()\n",
    "model = model.double().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was pre partitioned into training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name = join('/home/squirt/Documents/data/weather_data/', 'train_data.h5')\n",
    "test_name = join('/home/squirt/Documents/data/weather_data/', 'test_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to load hdf5 file of the processed weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5(filename:str):\n",
    "    '''\n",
    "    Load data from an HDF5 file and return a list of dictionaries.\n",
    "    Inputs:\n",
    "        - filename (str): Path to the HDF5 file.\n",
    "    Outputs:\n",
    "        - data (list): List of dictionaries, where each dictionary represents an entry in the original list.\n",
    "    '''\n",
    "    data = []  # List to hold dictionaries\n",
    "    with h5.File(filename, 'r') as f:\n",
    "        # Iterate through groups (each representing an entry in the original list)\n",
    "        for group_name in f:\n",
    "            group = f[group_name]\n",
    "            # Reconstruct dictionary from datasets and attributes\n",
    "            entry = {\n",
    "                # Attributes (metadata)\n",
    "                'day': group.attrs['day'],\n",
    "                'region': group.attrs['region'],\n",
    "                'time': group.attrs['time'],\n",
    "\n",
    "                # groups (numpy arrays)\n",
    "                'landmass': group['landmass'][...],  # Use [...] to read the full dataset\n",
    "                'x': group['x'][...],\n",
    "                'y': group['y'][...],\n",
    "            }\n",
    "            data.append(entry)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate stacks to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_data(data:list[dict], key:str) -> torch.Tensor:\n",
    "    return torch.stack([torch.tensor(entry[key]) for entry in data])\n",
    "\n",
    "\n",
    "def generate_stacks(data:list[dict]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Create a PyTorch DataLoader from the data.\n",
    "    Inputs:\n",
    "        - data (list): List of dictionaries, where each dictionary represents an entry in the original list.\n",
    "    Outputs:\n",
    "        - landmass (torch.Tensor): Tensor of landmass data.\n",
    "        - x (torch.Tensor): Tensor of x-coordinate data.\n",
    "        - y (torch.Tensor): Tensor of y-coordinate data.\n",
    "    '''\n",
    "    landmass = stack_data(data, 'landmass')\n",
    "\n",
    "    x = stack_data(data, 'x')\n",
    "    x = x.transpose(2, 1)\n",
    "\n",
    "    y = stack_data(data, 'y')\n",
    "    y = y.transpose(2, 1)\n",
    "    \n",
    "    return (landmass, x, y)\n",
    "\n",
    "\n",
    "class weather_dataset(Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset class for weather data.\n",
    "    '''\n",
    "    def __init__(self, data:list[dict]):\n",
    "        self.landmass, self.x, self.y = generate_stacks(data)\n",
    "        self.length = len(self.landmass)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.landmass[idx], self.x[idx], self.y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition training data into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(fname:str, batch_size:int, split:int) -> tuple[DataLoader, DataLoader]:\n",
    "    '''\n",
    "    Create PyTorch DataLoader objects for training and validation data.\n",
    "    Inputs:\n",
    "        - fname (str): Path to the HDF5 file.\n",
    "        - batch_size (int): Batch size for the DataLoader objects.\n",
    "        - split (float): Fraction of the data to use for training.     \n",
    "    Outputs:\n",
    "        - train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "        - test_loader (torch.utils.data.DataLoader): DataLoader for test data.\n",
    "    '''\n",
    "    # Load data and create tensor \n",
    "    data = load_hdf5(fname)\n",
    "    dataset = weather_dataset(data)\n",
    "    \n",
    "    train_size = int(split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training loop, use mixed percision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module, dl:torch.utils.data.DataLoader, optim:torch.optim, loss:nn.Module) -> float:\n",
    "    model.train()\n",
    "    total_loss = .0\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for _, (l, x, y) in enumerate(dl):\n",
    "        l = l.cuda()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        # Flatten and combine\n",
    "        l = l.view(-1, 3*2*2)\n",
    "        x = x.view(-1, 70*3*2*2)\n",
    "        x = torch.cat((l, x), 1)\n",
    "\n",
    "        y = y.view(-1, 70*2*2*2)\n",
    "\n",
    "        # Forward pass\n",
    "        optim.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            y_pred = model(x)\n",
    "            l = loss(y_pred, y)\n",
    "            total_loss += l.item()\n",
    "\n",
    "        # Preform backpass\n",
    "        scaler.scale(l).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "    \n",
    "    return total_loss / len(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model:nn.Module, dl:torch.utils.data.DataLoader, loss:nn.Module) -> float:\n",
    "    model.eval()\n",
    "    total_loss = .0\n",
    "\n",
    "    for _, (l, x, y) in enumerate(dl):\n",
    "        l = l.cuda()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        # Flatten and combine\n",
    "        l = l.view(-1, 3*2*2)\n",
    "        x = x.view(-1, 70*3*2*2)\n",
    "        x = torch.cat((l, x), 1)\n",
    "\n",
    "        y = y.view(-1, 70*2*2*2)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            y_pred = model(x)\n",
    "            l = loss(y_pred, y)\n",
    "            total_loss += l.item()\n",
    "\n",
    "    return total_loss / len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3 \n",
    "models = {}\n",
    "\n",
    "test_ds = weather_dataset(load_hdf5(test_name)) \n",
    "test_loader = DataLoader(test_ds, batch_size=4096, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "for i in range(folds):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = simpleNN()\n",
    "    model = model.double().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Get Dataloaders\n",
    "    train_loader, val_loader = get_dataloaders(train_name, 4096, 0.6)\n",
    "\n",
    "    # Early Stopping \n",
    "    eval_loss = -1*float('inf') \n",
    "    train_loss  = float('inf') \n",
    "    j = 0\n",
    "    print(f'Fold {i+1}')\n",
    "    while train_loss > eval_loss:\n",
    "        prev_eval_loss = eval_loss\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn)\n",
    "        eval_loss = eval(model, val_loader, loss_fn)\n",
    "        print(f'\\tEpoch {j} - Eval Loss: {eval_loss}')\n",
    "        \n",
    "        if abs(eval_loss - prev_eval_loss) < 0.01:\n",
    "            print(f'\\tEarly stopping at epoch {j} due to minimal change in eval loss')\n",
    "            break\n",
    "        j += 1\n",
    "\n",
    "    # Test Loss\n",
    "    test_loss = eval(model, test_loader, loss_fn)\n",
    "    print(f'Fold {i+1} - Test Loss: {test_loss}')\n",
    "\n",
    "    # Save Model\n",
    "    model.eval()\n",
    "    models[i] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_column(data, col):\n",
    "    s = data[:,col,:,0,0] # Slice column pull 1st cell for now, make random later\n",
    "    s = s.view(-1, 2)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cell(data_dict:dict, cell:int, idx:int):\n",
    "    if idx not in {0,1}:\n",
    "        raise ValueError('idx must be 0 or 1')\n",
    "    title = 'Temp' if idx == 0 else 'QV'\n",
    "\n",
    "    # Get the data for the 1st column (were just trying to determin if we want to park model for now)\n",
    "    data = []\n",
    "    for _,v in data_dict.items():\n",
    "        data.append(v[0][:,idx])\n",
    "\n",
    "    #print(data[0][0])\n",
    "    data = torch.stack(data)\n",
    "    #print(data[0,0])\n",
    "\n",
    "    # Calculate Mean and STD\n",
    "    mean = (data.mean(dim=0)).cpu().numpy()\n",
    "    std = (data.std(dim=0)).cpu().numpy()\n",
    "\n",
    "    # Get Real Values\n",
    "    real = (data_dict[0][1][:,idx]).cpu().numpy()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "            \n",
    "    # Plot the real values on the x-axis and the mean values on the y-axis\n",
    "    plt.errorbar(real, mean, yerr=std, fmt='o', color='b', label='Mean Prediction with Std Dev')\n",
    "\n",
    "    # Get the limits of the x-axis\n",
    "    x_start, x_end = plt.xlim()\n",
    "\n",
    "    # Add a line of slope 1\n",
    "    plt.plot([x_start, x_end], [x_start, x_end], color='green', linestyle='-', label='Line of slope 1')\n",
    "\n",
    "\n",
    "    plt.title(f'{title} Predictions vs True Values, Cell {cell}')\n",
    "    plt.xlabel('Real Value')\n",
    "    plt.ylabel('Mean Prediction')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(test_loader))\n",
    "l, x, y = batch_data\n",
    "l = l.float().cuda()\n",
    "x = x.float().cuda()\n",
    "y = y.float().cuda()\n",
    "\n",
    " # Combine\n",
    "l = l.unsqueeze(1)\n",
    "x = torch.cat((l, x), 1)\n",
    "\n",
    "model_preds = {}\n",
    "for i, model in models.items():\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(x)\n",
    "        model_preds[i] = (y_hat, y)\n",
    "    \n",
    "for i in range(70):\n",
    "    plot_values = {}\n",
    "    for k,v in models.items():\n",
    "        plot_values[k] = (slice_column(v[0], i), slice_column(v[1], i))\n",
    "    plot_cell(plot_values, i, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
