{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hero Run for Hybrid Model AI Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So input is 1x70x3x4 tensor, we calculate a standard deviation using a NN, then sample a Normal Distro using this output, do correction, then output sample for new mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.amp import autocast\n",
    "from torch.amp import GradScaler\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import h5py as h5\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do hybrid heterogenious computing, so assert GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available(), 'CUDA is not available.'\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthWiseConv1d(nn.Module):\n",
    "    def __init__(self, out_dim: int, input_shape: list[int] = [71, 3, 4]):\n",
    "        super(DepthWiseConv1d, self).__init__()\n",
    "        '''\n",
    "        Perform a depthwise 1D convolution over the last dimension (4) of the input tensor, \n",
    "        followed by a pointwise convolution to output a tensor with shape (-1, 71*3, out_dim).\n",
    "        Args:\n",
    "            - out_dim (int): Number of output features\n",
    "            - input_shape (list): Shape of the input tensor, default is [71,3,4] \n",
    "        '''\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        # Calculate the number of input channels for the convolution\n",
    "        in_channels = input_shape[0] * input_shape[1]  # 71 * 3\n",
    "\n",
    "        # Depthwise 1D convolution on the last dimension (4)\n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=in_channels,  # Depthwise keeps the number of channels the same\n",
    "            kernel_size=input_shape[2],  # 1D convolution kernel size\n",
    "            groups=in_channels  # Depthwise convolution\n",
    "        )\n",
    "\n",
    "        # Pointwise convolution to adjust to the desired output dimension\n",
    "        self.pointwise = nn.Conv1d(\n",
    "            in_channels=in_channels,  # 213 channels from the depthwise output\n",
    "            out_channels=in_channels,  # Keep the same number of channels\n",
    "            kernel_size=1  # Pointwise convolution\n",
    "        )\n",
    "\n",
    "        # Linear layer to map to the desired output dimension\n",
    "        self.linear = nn.Linear(in_channels, self.out_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Reshape input to [batch_size, in_channels, 4]\n",
    "        x = x.view(x.size(0), -1, x.size(-1))  # Adjust to [batch_size, 213, 4]\n",
    "\n",
    "        # Apply depthwise convolution\n",
    "        x = self.depthwise(x)\n",
    "\n",
    "        # Apply pointwise convolution\n",
    "        x = self.pointwise(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # Permute to shape [batch_size, sequence_length, in_channels]\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerTransformer(nn.Module):\n",
    "    def __init__(self, input_dim:int, output_dim:int, num_layers:int=2):\n",
    "        super(InnerTransformer, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            - input_dim (int): Number of input features\n",
    "            - output_dim (int): Number of output features\n",
    "            - num_layers (int): Number of transformer layers, default is 1\n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Build transformer layers\n",
    "        self.layers = nn.ModuleList([nn.TransformerEncoderLayer(d_model=self.input_dim, nhead=1) for _ in range(self.num_layers)]) # Make it simple for now\n",
    "\n",
    "        # Linear layer to output\n",
    "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTrans(nn.Module):\n",
    "    def __init__(self, input_shape:list[int]=[71,3,4], inner_shape:int=int(71*2), out_dim:int=int(2*4), num_layers:int=2):\n",
    "        super(ConvTrans, self).__init__()\n",
    "        '''\n",
    "        Build a DepthWiseConv1d model followed by a Transformer model followed by an output layer(tbd)\n",
    "        Args:\n",
    "            - input_shape (list): Shape of the input tensor, default is [71,3,4] (70 cells, 3 features, 1x4 grid, followed by spacial cell). \n",
    "            - inner_shape (int): Number of features to pass to the transformer, default is 70*2\n",
    "            - out_dim (int): Number of output features, default is 70*2*4 (70 cells, 2 features, 1x4 grid)\n",
    "            - num_layers (int): Number of transformer layers, default is 2\n",
    "        '''\n",
    "        self.depthwise = DepthWiseConv1d(out_dim=inner_shape, input_shape=input_shape)\n",
    "        self.transformer = InnerTransformer(input_dim=inner_shape, output_dim=out_dim, num_layers=num_layers)\n",
    "        self.output_cov = nn.Conv1d(in_channels=inner_shape, out_channels=70, kernel_size=1)\n",
    "        self.output_linear = nn.Linear(70, 70*out_dim)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.depthwise(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.output_cov(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.output_linear(x)\n",
    "        x = x.view(-1, 70, 2, 4) # Maybe make args??? IDK\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was pre partitioned into training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name = join('/home/squirt/Documents/data/weather_data/', 'train_data.h5')\n",
    "test_name = join('/home/squirt/Documents/data/weather_data/', 'test_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to load hdf5 file of the processed weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5(filename:str):\n",
    "    '''\n",
    "    Load data from an HDF5 file and return a list of dictionaries.\n",
    "    Inputs:\n",
    "        - filename (str): Path to the HDF5 file.\n",
    "    Outputs:\n",
    "        - data (list): List of dictionaries, where each dictionary represents an entry in the original list.\n",
    "    '''\n",
    "    data = []  # List to hold dictionaries\n",
    "    with h5.File(filename, 'r') as f:\n",
    "        # Iterate through groups (each representing an entry in the original list)\n",
    "        for group_name in f:\n",
    "            group = f[group_name]\n",
    "            # Reconstruct dictionary from datasets and attributes\n",
    "            entry = {\n",
    "                # Attributes (metadata)\n",
    "                'day': group.attrs['day'],\n",
    "                'region': group.attrs['region'],\n",
    "                'time': group.attrs['time'],\n",
    "\n",
    "                # groups (numpy arrays)\n",
    "                'landmass': group['landmass'][...],  # Use [...] to read the full dataset\n",
    "                'x': group['x'][...],\n",
    "                'y': group['y'][...],\n",
    "            }\n",
    "            data.append(entry)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate stacks to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_data(data:list[dict], key:str) -> torch.Tensor:\n",
    "    return torch.stack([torch.tensor(entry[key]) for entry in data])\n",
    "\n",
    "\n",
    "def generate_stacks(data:list[dict]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Create a PyTorch DataLoader from the data.\n",
    "    Inputs:\n",
    "        - data (list): List of dictionaries, where each dictionary represents an entry in the original list.\n",
    "    Outputs:\n",
    "        - landmass (torch.Tensor): Tensor of landmass data.\n",
    "        - x (torch.Tensor): Tensor of x-coordinate data.\n",
    "        - y (torch.Tensor): Tensor of y-coordinate data.\n",
    "    '''\n",
    "    landmass = stack_data(data, 'landmass')\n",
    "    landmass = landmass.view(-1, 3, 4)\n",
    "\n",
    "    x = stack_data(data, 'x')\n",
    "    x = x.transpose(2, 1)\n",
    "    x = x.view(-1, 70, 3, 4)\n",
    "\n",
    "    y = stack_data(data, 'y')\n",
    "    y = y.transpose(2, 1)\n",
    "    y = y.view(-1, 70, 2, 4)\n",
    "    \n",
    "    return (landmass, x, y)\n",
    "\n",
    "\n",
    "class weather_dataset(Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset class for weather data.\n",
    "    '''\n",
    "    def __init__(self, data:list[dict]):\n",
    "        self.landmass, self.x, self.y = generate_stacks(data)\n",
    "        self.length = len(self.landmass)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.landmass[idx], self.x[idx], self.y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition training data into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(fname:str, batch_size:int, split:int) -> tuple[DataLoader, DataLoader]:\n",
    "    '''\n",
    "    Create PyTorch DataLoader objects for training and validation data.\n",
    "    Inputs:\n",
    "        - fname (str): Path to the HDF5 file.\n",
    "        - batch_size (int): Batch size for the DataLoader objects.\n",
    "        - split (float): Fraction of the data to use for training.     \n",
    "    Outputs:\n",
    "        - train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "        - test_loader (torch.utils.data.DataLoader): DataLoader for test data.\n",
    "    '''\n",
    "    # Load data and create tensor \n",
    "    data = load_hdf5(fname)\n",
    "    dataset = weather_dataset(data)\n",
    "    \n",
    "    train_size = int(split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Normalize the x/y data, not the landmass, on train_loader.dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to have normalization layer (x) -> reshape -> NN -> reshape - > denorm layer (y). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizationLayer(nn.Module):\n",
    "    def __init__(self, data: torch.Tensor):\n",
    "        super(NormalizationLayer, self).__init__()\n",
    "\n",
    "        # Compute mean and std along the batch_size, x1, and x2 dimensions\n",
    "        self.mean = data.mean(dim=(0, 1), keepdim=True).to(device, dtype=dtype) \n",
    "        self.std = data.std(dim=(0, 1), keepdim=True).to(device, dtype=dtype) \n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        # Normalize the data\n",
    "        normalized_data = (x - self.mean) / (self.std + 1e-9)  # Add a small constant to avoid division by zero\n",
    "        return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenormalizationLayer(nn.Module):\n",
    "    def __init__(self, data: torch.Tensor):\n",
    "        super(DenormalizationLayer, self).__init__()\n",
    "\n",
    "        # Compute mean and std along the batch_size, x1, and x2 dimensions\n",
    "        self.mean = data.mean(dim=(0, 1), keepdim=True).to(device, dtype=dtype)  \n",
    "        self.std = data.std(dim=(0, 1), keepdim=True).to(device, dtype=dtype) \n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        # Denormalize the data\n",
    "        denormalized_data = x * (self.std + 1e-9) + self.mean\n",
    "        return denormalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReshapeLayer, self).__init__()\n",
    "\n",
    "    def reshape_x(self, l: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        l = l.unsqueeze(1)\n",
    "        x = torch.cat((l, x), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteModel(nn.Module):\n",
    "    def __init__(self, norm_layer:NormalizationLayer, reshape_layer:ReshapeLayer, neural_network:ConvTrans, denorm_layer:DenormalizationLayer):\n",
    "        super(CompleteModel, self).__init__()\n",
    "        self.norm_layer = norm_layer\n",
    "        self.reshape_layer = reshape_layer \n",
    "        self.neural_network = neural_network\n",
    "        self.denorm_layer = denorm_layer\n",
    "\n",
    "    def forward(self, x:torch.Tensor, l:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.norm_layer(x)\n",
    "        x = self.reshape_layer.reshape_x(l, x)\n",
    "\n",
    "        x = self.neural_network(x)\n",
    "        x = self.denorm_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Norm/Denorm x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(loader) -> CompleteModel:\n",
    "    norm_layer = NormalizationLayer(loader.dataset.dataset.x)\n",
    "    denorm_layer = DenormalizationLayer(loader.dataset.dataset.y)\n",
    "    reshape_layer = ReshapeLayer().to(device, dtype=dtype) \n",
    "\n",
    "    model = ConvTrans().to(device, dtype=dtype) \n",
    "\n",
    "    network = CompleteModel(norm_layer, reshape_layer, model, denorm_layer).to(device, dtype=dtype) \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training loop, use mixed percision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module, dl:torch.utils.data.DataLoader, optim:torch.optim, loss:nn.Module) -> float:\n",
    "    model.train()\n",
    "    total_loss = .0\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for _, (l, x, y) in enumerate(dl):\n",
    "        l = l.to(device, dtype=dtype) \n",
    "        x = x.to(device, dtype=dtype) \n",
    "        y = y.to(device, dtype=dtype) \n",
    "\n",
    "        optim.zero_grad()\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            y_pred = model(x, l)\n",
    "            l = loss(y_pred, y)\n",
    "            total_loss += l.item()\n",
    "\n",
    "        # Preform backpass\n",
    "        scaler.scale(l).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "    \n",
    "    return total_loss / len(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model:nn.Module, dl:torch.utils.data.DataLoader, loss:nn.Module) -> float:\n",
    "    model.eval()\n",
    "    total_loss = .0\n",
    "\n",
    "    for _, (l, x, y) in enumerate(dl):\n",
    "        l = l.to(device, dtype=dtype) \n",
    "        x = x.to(device, dtype=dtype) \n",
    "        y = y.to(device, dtype=dtype)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            y_pred = model(x, l)\n",
    "            l = loss(y_pred, y)\n",
    "            total_loss += l.item()\n",
    "\n",
    "    return total_loss / len(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\tEpoch 0 - Eval Loss: 0.1926664956542889\n",
      "\tEpoch 1 - Eval Loss: 0.1861629000527449\n",
      "\tEpoch 2 - Eval Loss: 0.18289211427688856\n",
      "\tEarly stopping at epoch 2 due to minimal change in eval loss\n",
      "Fold 1 - Test Loss: 0.18655011517324696\n"
     ]
    }
   ],
   "source": [
    "folds = 1 \n",
    "models = {}\n",
    "\n",
    "test_ds = weather_dataset(load_hdf5(test_name)) \n",
    "test_loader = DataLoader(test_ds, batch_size=256, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "for i in range(folds):\n",
    "    train_loader, val_loader = get_dataloaders(train_name, 256, 0.6)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = make_model(train_loader) \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Early Stopping \n",
    "    eval_loss = -1*float('inf') \n",
    "    train_loss  = float('inf') \n",
    "    j = 0\n",
    "    print(f'Fold {i+1}')\n",
    "    while train_loss > eval_loss:\n",
    "        prev_eval_loss = eval_loss\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn)\n",
    "        eval_loss = eval(model, val_loader, loss_fn)\n",
    "        print(f'\\tEpoch {j} - Eval Loss: {eval_loss}')\n",
    "        \n",
    "        if abs(eval_loss - prev_eval_loss) < 0.005:\n",
    "            print(f'\\tEarly stopping at epoch {j} due to minimal change in eval loss')\n",
    "            break\n",
    "        j += 1\n",
    "\n",
    "    # Test Loss\n",
    "    test_loss = eval(model, test_loader, loss_fn)\n",
    "    print(f'Fold {i+1} - Test Loss: {test_loss}')\n",
    "\n",
    "    # Save Model\n",
    "    model.eval()\n",
    "    models[i] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning a new Mean Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_dist(nn.Module):\n",
    "    def __init__(self, model:nn.Module):\n",
    "        super(sample_dist, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x:torch.Tensor, l:torch.Tensor) -> torch.Tensor:\n",
    "        std = self.model(x, l)\n",
    "\n",
    "        # If Negative fix\n",
    "        std = torch.abs(std)\n",
    "\n",
    "        # Create a normal distribution\n",
    "        mean = x[:, :, 0:2, :]\n",
    "        dist = Normal(mean, std)\n",
    "\n",
    "        sample = dist.sample()\n",
    "\n",
    "        # Could do Physics Check here\n",
    "        # TODO\n",
    "        return mean,sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "distros = {}\n",
    "for i,m in models.items():\n",
    "    distros[i] = sample_dist(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(test_loader))\n",
    "l, x, y = batch_data\n",
    "l = l.cuda()\n",
    "x = x.cuda()\n",
    "y = y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([278.2605, 277.6979, 279.9170, 280.3481], device='cuda:0',\n",
      "       dtype=torch.float64) \n",
      "tensor([280.4068, 276.9530, 281.1160, 279.5179], device='cuda:0',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model_preds = {}\n",
    "for i,dis in distros.items():\n",
    "    with torch.no_grad():\n",
    "        m,s = dis(x, l)\n",
    "        print(f'{m[0,0,0,:]} \\n{s[0,0,0,:]}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 saved successfully.\n"
     ]
    }
   ],
   "source": [
    "for i,m in models.items():\n",
    "    try:\n",
    "        script_model = torch.jit.script(m)\n",
    "        script_model.save(f\"./weights/sample_net_{i}.pt\")\n",
    "        print(f\"Model {i} saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model {i}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
