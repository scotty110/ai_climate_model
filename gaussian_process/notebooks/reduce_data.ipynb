{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have too much data to train GP. How can we reduce (this might not be needed for NN??)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from os.path import join\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(data_tuple:tuple[torch.tensor]) -> tuple[torch.tensor]:\n",
    "    '''\n",
    "    Add Landmass to x data. Return x,y tensors\n",
    "    Input:\n",
    "        - data_tuple (tuple[torch.tensor]): tuple of landmass,x,y tensors\n",
    "    '''\n",
    "    l,x,y = data_tuple\n",
    "\n",
    "    # Combine\n",
    "    l = l.unsqueeze(1)\n",
    "    x = torch.cat((l, x), 1)\n",
    "    x = x.contiguous()\n",
    "    y = y.contiguous()\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = join('/home/squirt/Documents/data/weather_data/', 'all_data.h5')\n",
    "data = utils.load_hdf5(fname)\n",
    "stacks = utils.generate_stacks(data)\n",
    "x, y = combine_data(stacks)\n",
    "del data, stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use the mean since this isn't scientific. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.mean(x, dim=(3,4))\n",
    "y = torch.mean(y, dim=(3,4)) \n",
    "\n",
    "x = x.view(-1, 71*3)\n",
    "y = y.view(-1, 70*2)\n",
    "\n",
    "x = x.numpy()\n",
    "y = y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster to remove dissimilar x pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is your feature matrix\n",
    "clustering = DBSCAN(eps=0.5, min_samples=5).fit(x)\n",
    "# Get the cluster labels\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Identify unique clusters (excluding noise points labeled as -1)\n",
    "unique_labels = set(labels) - {-1}\n",
    "\n",
    "# Select one representative point from each cluster\n",
    "representatives = []\n",
    "for label in unique_labels:\n",
    "    indices = np.where(labels == label)[0]\n",
    "    representatives.append(x[indices[0]])  # You can choose any point within the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So looks like noise, thus should use all data????"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
