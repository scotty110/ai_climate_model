{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition H5 File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing files using \"process_data.py\", need to partition into train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5\n",
    "import numpy as np\n",
    "\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = join('/home/squirt/Documents/data/weather_data/','all_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5(filename:str):\n",
    "    '''\n",
    "    Load data from an HDF5 file and return a list of dictionaries.\n",
    "    Inputs:\n",
    "        - filename (str): Path to the HDF5 file.\n",
    "    Outputs:\n",
    "        - data (list): List of dictionaries, where each dictionary represents an entry in the original list.\n",
    "    '''\n",
    "    data = []  # List to hold dictionaries\n",
    "    with h5.File(filename, 'r') as f:\n",
    "        # Iterate through groups (each representing an entry in the original list)\n",
    "        for group_name in f:\n",
    "            group = f[group_name]\n",
    "            # Reconstruct dictionary from datasets and attributes\n",
    "            entry = {\n",
    "                # Attributes (metadata)\n",
    "                'day': group.attrs['day'],\n",
    "                'region': group.attrs['region'],\n",
    "                'time': group.attrs['time'],\n",
    "\n",
    "                # groups (numpy arrays)\n",
    "                'landmass': group['landmass'][...],  # Use [...] to read the full dataset\n",
    "                'x': group['x'][...],\n",
    "                'y': group['y'][...],\n",
    "            }\n",
    "            data.append(entry)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = load_hdf5(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_data[:int(split*len(all_data))]\n",
    "test_data = all_data[int(split*len(all_data)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_h5(data:list[dict], filename:str):\n",
    "    '''\n",
    "    Saves a list of dictionaries to an HDF5 file, with dictionaries converted to groups.\n",
    "    Inputs:\n",
    "        - data (list[dict]): list of dictionaries to save with keys {day, region, time, landmass, x, y}\n",
    "        - filename (str): path to the file to save\n",
    "    Outputs:\n",
    "        - None, data saved to disk\n",
    "    ''' \n",
    "    with h5.File(filename, 'w') as f:\n",
    "        for i, entry in enumerate(data):\n",
    "            group = f.create_group(f'entry_{i}')\n",
    "            for key, value in entry.items():\n",
    "                if isinstance(value, np.ndarray):\n",
    "                    group.create_dataset(key, data=value)\n",
    "                else:\n",
    "                    # Store non-array data as attributes\n",
    "                    # Ensure that the value is converted to a string, as HDF5 attributes\n",
    "                    # are more versatile with string data types.\n",
    "                    group.attrs[key] = str(value)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = join('/home/squirt/Documents/data/weather_data/','train_data.h5')\n",
    "testing_file = join('/home/squirt/Documents/data/weather_data/','test_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_h5(train_data, training_file)\n",
    "save_h5(test_data, testing_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
